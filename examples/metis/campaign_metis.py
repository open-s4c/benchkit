#!/usr/bin/env python3
# Copyright (C) 2025 Vrije Universiteit Brussel. All rights reserved.
# SPDX-License-Identifier: MIT

"""
Example of campaign script for micro-benchmarking the VSync locks.
"""

from benchmarks.wrmem_bench import WrMemBench
from benchmarks.kmeans_bench import KMeansBench
from benchmarks.pca_bench import PCABench
from benchmarks.matmult_bench import MatrixMultBench
from benchmarks.hist_bench import HistBench
from benchmarks.linreg_bench import LinRegBench
from benchmarks.stringmatch_bench import StringMatchBench
from benchmarks.wc_bench import WCBench
from benchmarks.wr_bench import WRBench

from benchkit.campaign import CampaignCartesianProduct, CampaignIterateVariables, CampaignSuite
from benchkit.shell.shell import shell_out

NB_RUNS = 1
DURATION_SECONDS = None


# wrmem
WRMEM_NB_PROCS = 0 # of processors to use (use all cores by default
WRMEM_NB_TASKS_MAP = 0 #map tasks : # of map tasks (16 tasks per core by default
WRMEM_NB_TASKS_REDUCE = 0 #reduce tasks : # of reduce tasks (determined by sampling by default, disables sampling if not zero)
WRMEM_NB_TOP_VALUES_DISPLAYED = 0 # of top key/value pairs to display
WRMEM_INPUT_SIZE = 0 # of mega-bytes input
WRMEM_SILENT_RUN = True

# kmeans
KMEANS_VECTOR_DIM = 10
KMEANS_NB_CLUSTERS = 16
KMEANS_NB_POINTS = 5000000
KMEANS_MAX_VALUE = 40
KMEANS_NB_PROCS = 0 # of processors to use
KMEANS_NB_TASKS_MAP = 0 # of map mask (pre-split input before MR)
KMEANS_NB_TASKS_REDUCE = 0 # of reduce tasks
KMEANS_NB_TOP_VALUES_DISPLAYED = 0 # of top value pairs to display
KMEANS_SILENT_RUN = True

# pca
PCA_NB_PROCS = 0
PCA_NB_TASKS_MAP = 0
PCA_NB_TASKS_REDUCE = 0
PCA_NB_ROWS = 2048 # of rows in matrix
PCA_NB_COLS = 2048 # of columns in matrix
PCA_MATRIX_MAX = 100 # all values in the matrix are from 0 to this value
PCA_SILENT_RUN = True

# matrix_mult
MATMULT_NB_PROCS = 0
MATMULT_NB_TASKS_MAP = 0
MATMULT_MATRIX_LENGTH = 2048
MATMULT_SILENT_RUN = True

# hist
HIST_NB_PROCS = 0
HIST_NB_TASKS_MAP = 0
HIST_NB_TASKS_REDUCE = 0
HIST_TEST_RUN = False
HIST_SILENT_RUN = True

# linear_regression
LINREG_NB_PROCS = 0
LINREG_NB_TASKS_MAP = 0
LINREG_TEST_RUN = False
LINREG_SILENT_RUN = True

# string_match
STRINGMATCH_NB_PROCS = 0
STRINGMATCH_NB_TASKS_MAP = 0
STRINGMATCH_NB_TASKS_REDUCE = 0
STRINGMATCH_TEST_RUN = False
STRINGMATCH_SILENT_RUN = True

# wc
WC_NB_PROCS = 0
WC_NB_TASKS_MAP = 0
WC_NB_TASKS_REDUCE = 0
WC_NB_TOP_VALUES_DISPLAYED = 0
WC_TEST_RUN = False
WC_ALPHA_NUMERIC = False
WC_SILENT_RUN = True

# wr
WR_DATA_PATH_1 = "wr/100MB_1M_Keys.txt" # many keys and few duplicates
WR_DATA_PATH_2 = "wr/100MB_100K_Keys.txt" # few keys and many duplicates
WR_DATA_PATH_3 = "wr/800MB.txt" # many keys and many duplicates. The input is generated via data/data-gen.sh
WR_DATA_PATH_4 = "wr/500MB.txt"# many keys and many duplicates, but unpredictable. The input is generated by data/data-gen.sh
WR_NB_PROCS = 0
WR_NB_TASKS_MAP = 0
WR_NB_TASKS_REDUCE = 0
WR_NB_TOP_VALUES_DISPLAYED = 0
WR_SILENT_RUN = True


def campaign_wrmem() -> CampaignIterateVariables:
	kernel = shell_out("uname -r").strip()
	benchmark_name = "wrmem"
	return CampaignIterateVariables(
		name=f"campaign_{benchmark_name}",
		benchmark=WrMemBench(),
		nb_runs=NB_RUNS,
		variables=[
			{
				"benchmark_name": benchmark_name,
				"nb_procs": WRMEM_NB_PROCS,
				"nb_tasks_map": WRMEM_NB_TASKS_MAP,
				"nb_tasks_reduce": WRMEM_NB_TASKS_REDUCE,
				"nb_top_values": WRMEM_NB_TOP_VALUES_DISPLAYED,
				"input_size": WRMEM_INPUT_SIZE,
				"silent": WRMEM_SILENT_RUN,
			}
		],
		constants={
			"kernel": kernel,
		},
		debug=False,
		gdb=False,
		enable_data_dir=False,
		benchmark_duration_seconds=DURATION_SECONDS,
		# pretty={
		#     "lock": {
		#         "cas": "CAS spinlock",
		#         "ttas": "TTAS lock",
		#         "ticket": "Ticketlock",
		#     }
		# },
	)


def campaign_kmeans() -> CampaignIterateVariables:
	kernel = shell_out("uname -r").strip()
	benchmark_name = "kmeans"
	return CampaignIterateVariables(
        name=f"campaign_{benchmark_name}",
        benchmark=KMeansBench(),
        nb_runs=NB_RUNS,
        variables=[
            {
				"benchmark_name": benchmark_name,
				"vector_dim": KMEANS_VECTOR_DIM,
				"nb_clusters": KMEANS_NB_CLUSTERS,
				"nb_points": KMEANS_NB_POINTS,
				"max_value": KMEANS_MAX_VALUE,
				"nb_procs": KMEANS_NB_PROCS,
				"nb_tasks_map": KMEANS_NB_TASKS_MAP,
				"nb_tasks_reduce": KMEANS_NB_TASKS_REDUCE,
                "nb_top_values": KMEANS_NB_TOP_VALUES_DISPLAYED,
				"silent": KMEANS_SILENT_RUN,
			}
        ],
        constants={
            "kernel": kernel,
        },
        debug=False,
        gdb=False,
        enable_data_dir=False,
        benchmark_duration_seconds=DURATION_SECONDS,
	)


def campaign_pca() -> CampaignIterateVariables:
	kernel = shell_out("uname -r").strip()
	benchmark_name = "pca"
	return CampaignIterateVariables(
		name=f"campaign_{benchmark_name}",
		benchmark=PCABench(),
		nb_runs=NB_RUNS,
		variables=[
			{
				"benchmark_name": benchmark_name,
				"nb_procs": PCA_NB_PROCS,
				"nb_tasks_map": PCA_NB_TASKS_MAP,
				"nb_tasks_reduce": PCA_NB_TASKS_REDUCE,
				"nb_rows": PCA_NB_ROWS,
				"nb_cols": PCA_NB_COLS,
				"matrix_max": PCA_MATRIX_MAX,
				"silent": PCA_SILENT_RUN,
			}
		],
		constants={
			"kernel": kernel,
		},
		debug=False,
		gdb=False,
		enable_data_dir=False,
		benchmark_duration_seconds=DURATION_SECONDS,
	)


def campaign_matmult() -> CampaignIterateVariables:
	kernel = shell_out("uname -r").strip()
	benchmark_name = "matrix_mult"
	return CampaignIterateVariables(
		name=f"campaign_{benchmark_name}",
		benchmark=MatrixMultBench(),
		nb_runs=NB_RUNS,
		variables=[
			{
				"benchmark_name": benchmark_name,
				"nb_procs": MATMULT_NB_PROCS,
				"nb_tasks_map": MATMULT_NB_TASKS_MAP,
				"matrix_length": MATMULT_MATRIX_LENGTH,
				"silent": MATMULT_SILENT_RUN,
			}
		],
		constants={
			"kernel": kernel,
		},
		debug=False,
		gdb=False,
		enable_data_dir=False,
		benchmark_duration_seconds=DURATION_SECONDS,
	)


def campaign_hist() -> CampaignIterateVariables:
	kernel = shell_out("uname -r").strip()
	benchmark_name = "hist"
	return CampaignIterateVariables(
		name=f"campaign_{benchmark_name}",
		benchmark=HistBench(),
		nb_runs=NB_RUNS,
		variables=[
			{
				"benchmark_name": benchmark_name,
				"nb_procs": HIST_NB_PROCS,
				"nb_tasks_map": HIST_NB_TASKS_MAP,
				"nb_tasks_reduce": HIST_NB_TASKS_MAP,
				"test_run": HIST_TEST_RUN,
				"silent": HIST_SILENT_RUN,
			}
		],
		constants={
			"kernel": kernel,
		},
		debug=False,
		gdb=False,
		enable_data_dir=False,
		benchmark_duration_seconds=DURATION_SECONDS,
	)


def campaign_linear_reg() -> CampaignIterateVariables:
	kernel = shell_out("uname -r").strip()
	benchmark_name = "linear_regression"
	return CampaignIterateVariables(
		name=f"campaign_{benchmark_name}",
		benchmark=LinRegBench(),
		nb_runs=NB_RUNS,
		variables=[
			{
				"benchmark_name": benchmark_name,
				"nb_procs": LINREG_NB_PROCS,
				"nb_tasks_map": LINREG_NB_TASKS_MAP,
				"test_run": LINREG_TEST_RUN,
				"silent": LINREG_SILENT_RUN,
			}
		],
		constants={
			"kernel": kernel,
		},
		debug=False,
		gdb=False,
		enable_data_dir=False,
		benchmark_duration_seconds=DURATION_SECONDS,
	)


def campaign_string_match() -> CampaignIterateVariables:
	kernel = shell_out("uname -r").strip()
	benchmark_name = "string_match"
	return CampaignIterateVariables(
		name=f"campaign_{benchmark_name}",
		benchmark=StringMatchBench(),
		nb_runs=NB_RUNS,
		variables=[
			{
				"benchmark_name": benchmark_name,
				"nb_procs": STRINGMATCH_NB_PROCS,
				"nb_tasks_map": STRINGMATCH_NB_TASKS_MAP,
				"nb_tasks_reduce": STRINGMATCH_NB_TASKS_MAP,
				"test_run": STRINGMATCH_TEST_RUN,
				"silent": STRINGMATCH_SILENT_RUN,
			}
		],
		constants={
			"kernel": kernel,
		},
		debug=False,
		gdb=False,
		enable_data_dir=False,
		benchmark_duration_seconds=DURATION_SECONDS,
	)


def campaign_wc() -> CampaignIterateVariables:
	kernel = shell_out("uname -r").strip()
	benchmark_name = "wc"
	return CampaignIterateVariables(
		name=f"campaign_{benchmark_name}",
		benchmark=WCBench(),
		nb_runs=NB_RUNS,
		variables=[
			{
				"benchmark_name": benchmark_name,
				"nb_procs": WC_NB_PROCS,
				"nb_tasks_map": WC_NB_TASKS_MAP,
				"nb_tasks_reduce": WC_NB_TASKS_MAP,
				"nb_top_values": WC_NB_TOP_VALUES_DISPLAYED,
				"test_run": WC_TEST_RUN,
				"alpha_numeric": WC_ALPHA_NUMERIC,
				"silent": WC_SILENT_RUN,
			}
		],
		constants={
			"kernel": kernel,
		},
		debug=False,
		gdb=False,
		enable_data_dir=False,
		benchmark_duration_seconds=DURATION_SECONDS,
	)


def campaign_wr() -> CampaignCartesianProduct:
	kernel = shell_out("uname -r").strip()
	benchmark_name = "wr"

	data_paths = [
		WR_DATA_PATH_1,
		WR_DATA_PATH_2,
		WR_DATA_PATH_3,
		WR_DATA_PATH_4,
	]

	return CampaignCartesianProduct(
		name=f"campaign_{benchmark_name}",
		benchmark=WRBench(),
		nb_runs=NB_RUNS,
		variables={
			"data_path": data_paths,
		},
		constants={
			"kernel": kernel,
			"benchmark_name": benchmark_name,
			"nb_procs": WR_NB_PROCS,
			"nb_tasks_map": WR_NB_TASKS_MAP,
			"nb_tasks_reduce": WR_NB_TASKS_MAP,
			"nb_top_values": WR_NB_TOP_VALUES_DISPLAYED,
			"silent": WR_SILENT_RUN,
		},
		debug=False,
		gdb=False,
		enable_data_dir=False,
		benchmark_duration_seconds=DURATION_SECONDS,
	)


def main() -> None:
    """Main function of the campaign script."""
    # kernel = shell_out("uname -r").strip()

    campaigns = [
		# campaign_wrmem(),
		# campaign_kmeans(),
		# campaign_pca(),
		# campaign_matmult(),
		# campaign_hist(),
		# campaign_linear_reg(),
		# campaign_string_match(),
		# campaign_wc(),
		# campaign_wr(),
	]
    suite = CampaignSuite(campaigns=campaigns)
    suite.print_durations()
    suite.run_suite()
    pass


if __name__ == "__main__":
    main()
